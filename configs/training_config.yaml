# configs/training_config.yaml
# Global config for penny_ai, with per-model overrides where necessary

paths:
  data_root:        "./data"
  metadata:         "./metadata/metadata.json"
  roi:              "./metadata/roi_coordinates.json"
  models:           "./output/models_pytorch" # For PyTorch model checkpoints
  logs:             "./output/logs"           # For training logs
  exports:          "./output/exports"        # For ONNX models and other exported artifacts

# Paths specifically for running tests
test_paths:
  data_root:        "./data_examples"         # Test data
  metadata:         "./metadata/metadata.json"  # Can be the same or a test-specific one
  roi:              "./metadata/roi_coordinates.json" # Can be test-specific
  models:           "./output_test/models_pytorch"    # Default test output for PyTorch models, overridden by tmp_path in smoke test
  logs:             "./output_test/logs"      # Default test output for logs, overridden by tmp_path in smoke test
  exports:          "./output_test/exports"   # Default test output for exports, overridden by tmp_path in smoke test

# Global defaults for data, augmentation, training, and execution
defaults:
  data:
    image_size:      [224, 224] # Default image size, can be overridden per model
    normalization:
      mean:        [0.5, 0.5, 0.5]
      std:         [0.5, 0.5, 0.5]
    roi_padding:
      date:  { top: 0.0, bottom: 0.0, left: 0.0, right: 0.0 }
      mint:  { top: 0.0, bottom: 0.0, left: 0.0, right: 0.0 }

  augmentation:
    apply_augmentations:    true
    random_horizontal_flip: true
    rotation_degrees:       15
    translate:              [0.1, 0.1]
    scale:                  [0.9, 1.1]
    shear:                  5
    random_erasing_prob:    0.5
    random_erasing_scale:   [0.02, 0.1]
    random_erasing_ratio:   [0.3, 3.3]
    gaussian_noise_std:     0.05

  training:
    epochs:                 100
    batch_size:             32
    learning_rate:          0.0003  # default LR for most models
    optimizer:              "adamw"
    weight_decay:           0.0001
    scheduler:              "cosine" # e.g., "cosine", "step", "plateau"
    warmup_epochs:          5
    early_stopping:
      patience:             10 # Number of epochs to wait for improvement
      min_delta:            0.001 # Minimum change to qualify as an improvement
    save_every_n_epochs:    10

  execution:
    seed:                   42
    device:                 "auto" # "cuda", "cpu", or "auto"
    num_workers:            4

# Per-model configuration: only fields differing from defaults are specified
models:
  side:
    data:
      image_size:         [224, 224]
    training:
      batch_size:         8      # specific for side model
      learning_rate:      0.0001 # side requires a lower LR
    augmentation:
      apply_augmentations: true

  date:
    data:
      image_size:         [96, 96]
    training:
      batch_size:         64
      # learning_rate uses default 0.0003
    augmentation:
      apply_augmentations: true

  mint:
    data:
      image_size:         [32, 32]
    training:
      batch_size:         64
    augmentation:
      apply_augmentations: true

  orientation:
    data:
      image_size:         [128, 128]
    training:
      batch_size:         64
    augmentation:
      apply_augmentations: false # orientation skip text/shape-disrupting augs
